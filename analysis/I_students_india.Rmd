---
title: "I_students_india"
author: "Anne"
date: "12/20/2021"
output: html_document
---

# I Students and India data study 


```{r}
`%>%` <- magrittr::`%>%`
comp_dat <- events_studis_india %>%
  dplyr::select(matches("t1emotions|jobstr|jobsa|t1threat|gender|age|found|own|occ|lang|edu|
                  preocc|t1cope|t1novel|t1disrup|t1cope|t1perfo|probsolv|max_sev|severity"))


alph_dat <- events_studis_india %>%
  dplyr::select(matches("t1emotions|jobstr|t1threat"))


comp_split <- comp_dat %>%
  split.default(sub("_.*", "", names(comp_dat))) 

alph_split <- alph_dat %>%
  split.default(sub("_.*", "", names(alph_dat))) 
library(tidyverse)

comp <- map(comp_split, ~ multicon::composite(.x, nomiss = 0.8), data = .x) %>% as.data.frame(.)
alph <- map(alph_split, ~ psych::alpha(.x), data = .x) %>%
  map(~ .x$total)

alph_df <- do.call("rbind", alph) %>% round(., 2)

```

## Reliabilities

``` {r reliabilities, include = T, echo = F}
alph_df %>%
DT::datatable(
    extensions = 'Buttons', 
    options = list(dom = 'Bfrtip', 
                   buttons = c('excel', "csv"),
                   pageLength = 20))
```

## Correlations

```{r}
cor <- round(cor(comp, use="pairwise.complete.obs"), 2)
library(kableExtra)

corstar_select <- data.frame(corstars(comp, removeTriangle = "upper", result="none"))


corstar_select %>%
  DT::datatable(
    extensions = 'Buttons', 
    options = list(dom = 'Bfrtip', 
                   buttons = c('excel', "csv"),
                   pageLength = 35,
                  lengthMenu = c(25, 50, 75, 94)))

```

## Severity > Emotions > Coping

```{r}
library(lavaan)
set.seed(1234)
model <- ' # direct effect
             t1copeact ~ c*severity
           # mediator
             t1emotionslow ~ a*severity
             t1copeact ~ b*t1emotionslow
             ab := a*b
           # total effect
             total := c + (a*b)
         '
fit <- sem(model, data = comp)
summary(fit)

names(comp)

```

## Severity > Emotions > Threat > Coping

```{r}
library(lavaan)
set.seed(1234)
model <- ' # direct effect
             t1copeact ~ c*t1threat + t1emotionshigh + d*severity 
           # mediator 1
             t1emotionslow ~ a*severity
             
          # mediator 2
             t1threat ~ b*t1emotionshigh + severity
 
             abc := a*b*c
           # total effect
             total := d + (a*b*c)
         '
fit <- sem(model, data = comp)
summary(fit)

names(comp)
```


```{r}
### Severity * Emos > Emocope 

library(semTools)
names(comp)

comp$severity_center <- scale(comp$severity, center = TRUE, scale = TRUE) 
comp$t1emotionshigh_center <- scale(comp$t1emotionshigh, center = TRUE, scale = TRUE) 

model1 <- lm(t1copeblame ~  severity_center, data = comp)
model2 <- lm(t1copeblame ~  t1emotionshigh_center, data = comp)
model3 <- lm(t1copeblame ~  severity_center*t1emotionshigh_center, data = comp)

summary(model1)
summary(model2)
summary(model3)
```


## Text mining 

### Cleaning up data for text mining

```{r}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library("qdap")

# annotate text
library("udpipe")
library("textrank")
library("textstem")


ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = df_events$t1evdes)
x <- as.data.frame(x)

# show most common nouns
stats <- subset(x, upos %in% c("NOUN", "ADJ"))

stats2 <- stats %>% 
     dplyr::group_by(doc_id) %>% 
     dplyr::mutate(sentences = paste0(token, collapse = " ")) 

evdes_nouns <- stats2[!(duplicated(stats2$sentences)|duplicated(stats2$sentences)),] %>% dplyr::select(sentences)

evdes <- evdes_nouns$sentences
evdes_1 <- VectorSource(evdes)
TextDoc <- Corpus(evdes_1)

#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")

# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
#TextDoc <- tm_map(TextDoc, removeWords, c(
  # abstract words
#  "one", "new", "got", "get", "kept", "done", "much", "part", "us", "work", "use", "issue", "problem", "end", "job", "lack", "difficulty", "day", "task", "bad", "certain", "development", "low", "difficult", "test", "last", "potential", "unable", "many", "enough", "right", "little", "flow", "relate", "kind", "percent", "start", "give", "sure", "different", "good", "hard", "like", "wish", "yes", "rend", "matter", "altigether", "lakh", "tracter", "jlcpcb", "pcb", "proper", "difference", "lin", "double", "past", "possible", "pull", "regain", "superior", "act", "live", "critical", "bother", "set", "hes", "fine", "â€˜", "wasn", "aware", "poc", "long", "free", "several", "lot", "opinion", "serious", "front", "exceptional", "option", "particular", "circumstance", "primary",  "complete", "altogether", "single", "audit", "extent", "squeeze", "wrong", "necessary", "moment", "thing", "overall", "heavy", "need", "common", "awful", "short", "main", 
  # specific words
 # "oil", "tech", "face", "forest", "netherlands", "concert", "video", "music", "euphorbia", "plant", "royleana", "eukalyptus", "brick",  "extaract", "extraction", "fuel", "tractor", "tree", "waste", "cloud", "hand", "unawares", "ecosystem", "hike", "toll", "ppt", "farmer", "bite", "signature", "chicken", "pasta", "	spin-dry", "cafe", "drink", "food", "fun", "stuff", "allergic", "film", "mumbai"
#  ))
# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
# Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
# Text stemming - which reduces words to their root form
#TextDoc <- tm_map(TextDoc, stemDocument)
TextDoc <- tm_map(TextDoc, lemmatize_strings)

TextDoc[[15]][1]
```


### Build the document martix

```{r}
library(tm)
# Build a term-document matrix
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
TextDoc_dtm <- DocumentTermMatrix(TextDoc)
TextDoc_tdm <- removeSparseTerms(TextDoc_tdm, .99)
TextDoc_dtm <- removeSparseTerms(TextDoc_dtm, .99)

dtm_m <- as.matrix(TextDoc_tdm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)

# remove sparse terms


```

### Word cloud and most common nouns

```{r}
library(wordcloud)
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          scale=c(3,.4), 
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

# show most common nouns
stats <- subset(x, upos %in% c("NOUN", "ADJ"))
stats <- txt_freq(x = stats$lemma)
library(lattice)

dtm_d$word <- factor(dtm_d$word, levels = rev(dtm_d$word))
dtm_head <- head(dtm_d, 22)
barchart(word ~ freq, data = dtm_head, col = "cadetblue", main = "Most occurring nouns and adjectives", xlab = "Freq")
```

### Show connections

```{r}
findAssocs(TextDoc_tdm, "time", corlimit = 0.4)
dtm_top <- removeSparseTerms(TextDoc_tdm, sparse = .97)
TextDoc_tdm_m <-  as.matrix(dtm_top)
distance <- dist(TextDoc_tdm_m, method = "euclidean")
fit <- hclust(distance, method = "complete")
plot(fit)

ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = df_events$t1evdes)
x <- as.data.frame(x)

# show most common nouns
stats <- subset(x, upos %in% c("NOUN", "ADJ"))
stats2 <- stats %>% 
     dplyr::group_by(doc_id) %>% 
     dplyr::mutate(sentences = paste0(token, collapse = " ")) 

evdes_nouns <- stats2[!(duplicated(stats2$sentences)|duplicated(stats2$sentences)),] %>% dplyr::select(sentences)

library(tidytext)
evdes_nouns %>% 
  unnest_tokens(word, sentences, token = "ngrams", n = 2) %>% 
  separate(word, c("word1", "word2"), sep = " ") %>%
  unite(word,word1, word2, sep = " ") %>% 
  dplyr::count(word, sort = TRUE) %>% 
  slice(1:10) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top Bigrams of Medium iOS App Reviews",
       subtitle = "using Tidytext in R",
       caption = "Data Source: itunesr - iTunes App Store")
```


### Topic modeling

#### 11 Topics 

```{r}
library(topicmodels)

rowTotals <- apply(TextDoc_dtm , 1, sum)
TextDoc_dtm   <- TextDoc_dtm[rowTotals> 0, ] 
# set a seed so that the output of the model is predictable
ap_lda <- LDA(TextDoc_dtm, k = 11, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 4) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

#### 10 Topics 

```{r}
library(topicmodels)

rowTotals <- apply(TextDoc_dtm , 1, sum)
TextDoc_dtm   <- TextDoc_dtm[rowTotals> 0, ] 
# set a seed so that the output of the model is predictable
ap_lda <- LDA(TextDoc_dtm, k = 10, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 4) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

#### 9 Topics 

```{r}
library(topicmodels)

rowTotals <- apply(TextDoc_dtm , 1, sum)
TextDoc_dtm   <- TextDoc_dtm[rowTotals> 0, ] 
# set a seed so that the output of the model is predictable
ap_lda <- LDA(TextDoc_dtm, k = 9, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 4) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```


#### 8 Topics 

```{r}
library(topicmodels)

rowTotals <- apply(TextDoc_dtm , 1, sum)
TextDoc_dtm   <- TextDoc_dtm[rowTotals> 0, ] 
# set a seed so that the output of the model is predictable
ap_lda <- LDA(TextDoc_dtm, k = 8, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 4) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

#### 7 Topics 

```{r}
library(topicmodels)
# set a seed so that the output of the model is predictable
ap_lda <- LDA(TextDoc_dtm, k = 7, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 4) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

#### 6 Topics 

```{r}
library(topicmodels)
# set a seed so that the output of the model is predictable
ap_lda <- LDA(TextDoc_dtm, k = 6, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

#### 5 Topics 

```{r}
library(topicmodels)
# set a seed so that the output of the model is predictable
ap_lda <- LDA(TextDoc_dtm, k = 4, control = list(seed = 1234))
ap_lda
#> A LDA_VEM topic model with 2 topics.
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 3) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
